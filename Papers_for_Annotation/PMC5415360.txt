ENFORCING CO-EXPRESSION IN MULTIMODAL REGRESSION FRAMEWORK
We consider the problem of multimodal data integration for the study of complex neurological diseases (e.g. schizophrenia). Among the challenges arising in such situation, estimating the link between genetic and neurological variability within a population sample has been a promising direction. A wide variety of statistical models arose from such applications. For example, Lasso regression and its multitask extension are often used to fit a multivariate linear relationship between given phenotype(s) and associated observations. Other approaches, such as canonical correlation analysis (CCA), are widely used to extract relationships between sets of variables from different modalities. In this paper, we propose an exploratory multivariate method combining these two methods. More Specifically, we rely on a â€™CCA-typeâ€™ formulation in order to regularize the classical multimodal Lasso regression problem. The underlying motivation is to extract discriminative variables that display are also co-expressed across modalities. We first evaluate the method on a simulated dataset, and further validate it using Single Nucleotide Polymorphisms (SNP) and functional Magnetic Resonance Imaging (fMRI) data for the study of schizophrenia.
1. Introduction
An increasing amount of high-dimensional biomedical data such as micro arrays (mRNA, SNP) or brain imaging sequences (MRI, PET) is collected every day. Classical unimodal analysis often ignore the potential joint effects that may exist, for example, between genes and specific brain regions for diseases such as Schizophrenia, Alzheimer, etc. By harnessing these joint effects across modalities, we might be able to identify new mechanisms that uni-modal methods may fail to capture. Imaging genomics is an emerging field whose aim is precisely to leverage the wealth of biomedical knowledge provided by genomic and brain imaging data. Integrating such multimodal data sets is critical to extract meaningful bio-markers, improve clinical outcome prediction or identify potential associations across modalities. Unfortunately, as mentioned by Lin, such studies using genomic and brain imaging data often run into two limitations: The first one is an average small sample size, which may result in over fitting issues. In order to address such constraint, many authors relied on the use of sparse models. One classical method introduced by Tibshirani is the Lasso regression. The second limitation is poor biomarker reproducibility across studies. Although this issue remains an open problem, one may hope that using appropriate priors over the solution will lead to an improved consistency of the result across different studies.
1.1. Motivation: the study of Schizophrenia Schizophrenia
is a serious neurological disorder that affects around 1% of the general population. It is regarded as the result of various factors including genetic variants, brain development abnormalities and environmental effects. Identifying critical genes or SNPs related to schizophrenia has been a challenging issue. Many studies relied as well on brain imaging techniques to pinpoint functional abnormalities in brain regions for schizophrenia patients. Multimodal analysis (e.g. using both genomic and brain imaging) often improve generalization in situations in which many irrelevant features are present. In their recent paper, Cao et al. proposed a sparse representation based variable selection (SRVS) algorithm relying on sparse regression model to integrate both SNP and fMRI in order to perform biomarker selection for the study of schizophrenia. Lin proposed a group sparse canonical correlation analysis (CCA) method based on SNP and fMRI data to extract correlation between genes and brain regions. Le Floch et al. combined univariate filtering and Partial Least Squares (PLS) to identify SNPs covarying with various neuroimaging phenotypes. It appears that both regression and CCA methods display promising behaviors when combining SNP and fMRI data for the study of schizophrenia. In this work, we will try to merge these two methods in order to make the most out of both formulations.
The rest of this paper is organized as follows: we introduce in Section 2 some of the relevant methods as well as the motivation for this work. A novel approach to multivariate regression problems is then proposed in Section 3. Such method is then evaluated on both synthetic and real datasets in Section 4, followed by some discussions and concluding remarks in Section 5.
2. Methods
2.1. Learning with L1 penalty
We consider M âˆˆ â„•+ distinct (i.e. from different modalities) datasets with n samples and pm âˆˆ â„•+ (m = 1, â€¥, M) variables each. The m-th dataset is represented by a matrix Xm âˆˆ â„nÃ—pm. Additionally, each sample is assigned a class label (e.g. case/controls) yi âˆˆ {âˆ’1, 1}, i = 1, â€¥, n. Our goal is to look for a linear link between those class labels and the M data matrices. Let us consider the following regression model:  
The model described by Eq. 1 performs both variable selection and regularization. It often improves the prediction accuracy and interpretability of the results compared to the use of classical â„“2 norm regularization terms, especially when the number of variables is far greater than the number of observations. In some situations, we have several output vectors ym, âˆ€m = 1, â€¥, M and the m datasets are from the same modality: multi-task Lasso was proposed to capture shared structures among the various regression vectors. We consider the following model:  where P is the dimension of the problem and Î²p is the p-th row of the matrix such that Î² = [Î²1, â€¥, Î²m] (i.e. the Î²m are stacked horizontally). Such norm is also referred to as the â„“1/â„“2 norm, and is used to both enforce joint sparsity across the multiple Î²m and estimate only a few non-zero coefficients. Enforcing regularity within a modality (and across tasks) has been an active aspect of regression models, and has proven to increase reliability and results. However, since often pair-wise closeness is looked for in the common subspace, such methods will often fail to capture relationships across modalities.
2.2. Collaborative learning
Collaborative (or Co-regularized) methods are based on the optimization of measures of agreement and smoothness across multi-modal datasets. Smoothness across modalities is enforced through a joint regularization term. Their general model can be expressed as follows:  where the Um, m = 1, â€¥, M are arbitrary matrices whose roles are to control the cross-view joint regularization between each pair of vectors (Î²m, Î²q), m, q = 1, â€¥, M. Scalar parameter Î³ â‰¥ 0 controls the influence of such cross-regularization term. Notice that if Î³ = 0, we fall back on the original Lasso formulation. Collaborative learning is an interesting extension of Eq.1 allowing the user to explicitly enforce regularization across modalities. In this work, we rely on a special case of collaborative methods (introduced later in section 3) to address the following aspects: (i) Extend the regularization idea across modalities, (ii) Assume that relationships between variable are not available as a prior knowledge (as opposed, e.g., to Xin), (iii) Define links between components using correlation measure. To do so, we first briefly introduce in the next section some of the classical methods to extract meaningful relationships between variables across modalities.
2.3. Extracting relationship between datasets
A wide variety of problems amount to the joint analysis of multimodal datasets describing the same set of observations. Often, a mean to perform such analysis is to learn projection subspaces using paired samples such that structures of interest appear more clearly. Some of these methods are for example: Canonical correlation analysis (CCA), Partial least squares (PLS) or cross-modal factor analysis (CFA). Among them, CCA is probably the most widely used. Its goal is to extract linear combinations of variables with maximal correlation between two (or more) datasets. Using similar notations as in the previous section, and assuming M = 2, one formulation of CCA is expressed as follow:  to which a constraint on the norm of canonical vectors Î²1, Î²2 is added to avoid the trivial null solution. In recent years, CCA has been widely applied to genomic data analysis. As a consequence, many studies on sparse versions of CCA (sCCA) have been proposed to cope with the high dimension but low sample size problem. In the next section, we will rely on a CCA term to measure co-expression between variables from different modalities.
3. Enforcing cross-correlation in regression problems
3.1. MT-CoReg formulation
As discussed in Section 1, several methods have been proposed to: (i) Associate a phenotype and datasets while enforcing prior over solution, (ii) Extract relationships between coupled or co-expressed datasets. In the present study, we propose to associate both the regression and CCA frameworks in the case of M = 2 datasets. Our motivation is to extract informative features that also display a significant amount of correlation across modalities. A simple way to combine Lasso and sparse CCA would be a weighted combination of Eq.(1) and Eq.(4):  where Î³ âˆˆ [0, 1] is a weight parameter. Notice that Eq.(5) can be expressed within the collaborative framework introduced in Section 2.2. If we take a look at Eq.(3) with M = 2, U1 = X1 and U2 = X2, we fall back on Eq.(5). Let us call this model CoReg for Collaborative Regression. Interestingly, a similar model has been considered before by Gross to perform prediction using breast cancer data. However, to our opinion, such formulation might prove to be too constraining. It essentially amounts to force each component of the Î²mâ€™s to fit both the regression term and the CCA one. We illustrate such behaviour using a toy dataset later in Section 3.4. Since our goal is to perform feature selection, we may allow the model to be slightly more flexible. We thus propose an alternative formulation by first duplicating each Î²m into two components such that:  where Î±m, Î¸m are vectors from â„pm. As a consequence, the Î²mâ€™s are now matrices such that Î²m âˆˆ â„pmÃ—2 âˆ€m = 1, 2. We then propose the following MT-CoReg formulation:  where  is the i-th row of Î²m, i.e. . The third term of Eq.(3.3) is simply the â„“1/â„“2 norm of each of the Î²m. As we can observe from looking at Eq.(3.3), each â€™componentâ€™ (i.e. column of Î²m) will be involved in separate parts of the functional J: (i) components Î±m are the fit to the regression term of Eq.(3.3), (ii) components Î¸m are the fit of the CCA term of Eq.(3.3). Each pair (Î±m, Î¸m) and m = 1, 2 is coupled through the use of the â„“1/â„“2 norm from the third term in Eq.(3.3). Although their values are different, shared sparsity patterns are encouraged within each pair (Î±m, Î¸m). As a consequence, we allow the method to be significantly more flexible in terms of solutions: different values can be taken to simultaneously fit the Regression and CCA parts. We hope that such framework will encourage the selection of features that are discriminative (via the regression part) but also co-expressed across modalities (via the CCA part). Note that when Î³ = 0, criterion (3.3) essentially reduces to the initial regression problem of Eq.(1), while setting Î³ = 1 amounts to solving a conventional sparse CCA problem. A schematic view of the MT-CoReg pipeline can be seen in Fig.(1). In the next section, we briefly explain how to solve the problem described in Eq.(3.3).
3.2. Optimization
Initialization: estimate initial values for Î±1, Î²1, Î±2, Î²2 using ridge regression and ridge CCA.
Assume Î²1â€™s value fixed, and update Î²2 using Eq.(8).
Assume Î²2â€™s value fixed, and update Î²1 using the adapted version of Eq.(8).
Go back to step 2. until convergence
We solve the problem from Eq.(3.3) by optimizing the Î²mâ€™s alternatively over iterations until convergence, in a similar fashion to Wilms et al. formulation of sCCA. Suppose we have an initial value  for Î²1, and want to estimate Î²2. Updating matrix Î²2 can be recast into a problem of the following form:  where  Obviously, Eq.(8) is a classical group-lasso regression problem (cf. Eq.(2)). It is easy to show that updating Î²1 reduces to solving a similar problem. As a consequence, solving our mixed Lasso/CCA problem from Eq.(3.3) can be briefly summarized as:  
3.3. Parameter selection
Solving problem from Eq.(3.3) requires the estimation of two parameters, Î» and Î³, which respectively control the weights of the sparsity and the co-expression regularization terms. The choice of sparsity parameter Î» for this type of problems is known to display a high sensitivity. In order to make the searching process more robust, we chose to let the sparsity level of the solution control the tuning parameter value. Consider a column vector Î² âˆˆ â„p (e.g. a column of Î² from Eq.): let us denote |Î²|Îº the Îº-th (Îº âˆˆ â„•+) largest absolute magnitude of Î». We can Define a correspondence between Î» and Îº by making sure that for each iteration, we have Î» âˆˆ [|Î²|Îº, |Î²|Îº+1]. The selection can be looked for around the sample size (i.e. Îº = n for the entire estimation process), which helps drastically stabilize the estimation process in practice.
As for the estimation of Î³, we chose to rely on a technique introduced by Sun et al. based on variable selection stability. Its main goal is to select a given tuning parameter so that the associated variable selection method (in our case, the model from Eq.(3.3)) is stable in terms of the features it selects. In this framework, the training set is split in two halves using resampling (bootstrap resampling in our case). The variable selection method is then applied to each of the subsamples along a grid of candidate values for the parameter. Kappa selection criterion is then used to measure the degree of agreement between the two sets of variables obtained for a given parameter value. This process is then repeated a number of times, and an approximated measure of selection consistency is derived. The parameter value for which this consistency is the highest (after correction for the number of non-zeros elements retained) is the one kept for the estimation.
3.4. MT-CoReg VS. CoReg
As mentioned earlier in Section 3.1, in their CoReg model from Eq.(5) Gross et al. did not separate the solution vectors Î²m into two components. We then propose to illustrate the behavior of both models (Eq.(5) and Eq.(3.3)) on a toy dataset.
We generated M = 2 data matrices X1, X2 such that p1 = p2 = 30 and n = 50 observations. We used a latent variable model to simulate cross-correlated components so that columns p = [1, â€¥5]âˆª[10, â€¥15] of X1,X2 are mutually co-expressed. We further use columns p = [10, â€¥ 15]âˆª[20,â€¥25] to generate a phenotype vector y such that yi âˆˆ {âˆ’1; 1}. With such setup, columns p = [10, â€¥ 15] correspond to both non-zeros values in the true regression and canonical coefficients. Furthermore, let us point out that these non-zero values are diferent (canonical coefficientsâ€™ amplitude is lower than the regression ones). This setup can be seen in the first row of Fig.(2, Truth), where the blue and red curves are the values taken by the canonical and regression coefficients respectively. Resulting estimates for sCCA, Lasso, CoReg as well as proposed method MT-CoReg can also be seen in Fig.(2). In such scenario, while CoReg model assumes that regression and canonical coefficients have identical values, MT-CoReg has a wider scope and allows a finer joint estimation of both components types.
4. Experiments
In this section, we evaluate the proposed estimator from Eq.3.3. Performances will be assessed in terms of feature selection relevance on both simulated and real data.
4.1. Results on synthetic data
For our first test, we simulate both fMRI and SNP datasets. Similar to the toy dataset from Section 3.4, we start by generating explanatory variables  for both genomic and brain imaging data. The first 100 components of  are drawn from Normal distribution, while the rest is set to zero. The total number of observations is set to n = 200. Genomic values are coded as 0 (no minor allele), 1 (one minor allele), and 2 (two minor allele). We first Define a minor allele frequency Î· drawn from a uniform distribution ð’°([0.2, 0.4]). The i-th SNP is then generated from a binomial distribution â„¬(2, Î·i). For the imaging data, voxels values were drawn from a Gaussian distribution ð’©(0, Ip). Finally, binary phenotype y data are generated from â„¬(1, di), where . Furthermore, we add 100 additional variables to the problem that will play the role of cross-correlated variables. Two canonical vectors  are drawn from Normal distribution. Cross-correlated SNP are drawn from â„¬(2, logitâˆ’1(âˆ’ai + logit(Î·i))) where a is issued from , while cross-correlated voxels are drawn from . The final dataset is made of n = 200 observations of p = 1000 variables for both SNP and fMRI. Each of these datasets is made of explanatory and cross-correlated components. A common way to assess the performance of a model when it comes to feature selection is to measure the true positive rate (TPR) and false positive rate (FPR). TPR reflects the proportion of variables that are correctly identified, while FDR reflects the proportion of variables that are incorrectly selected by the model. We apply MT-CoReg to 100 random generation of the dataset described above. The tuning parameter Î³ from Eq.(3.3) that weights the CCA term against the regression one is optimized through a grid search over {[0] âˆª [10âˆ’1+â„“/20], â„“ = 0, â€¥, 20}. We plotted TPR values against FDR ones in Fig.(3) for two different cases. In the first (left) subfigure are displayed TPR/FDR values relative to non-zero components of  for Î³ = 0 (i.e. classical Lasso), Î³ = Î³(C.S.) where the weight value is determined using consistency selection (C.S.) scheme described in Section 3.3, and Î³ = 1 (i.e. classical sCCA). We can observe that although classical regression seems to perform slightly better for really low FDR values, MT-CoReg is quickly catching up around FDR â‰ˆ 0.15. sCCA, on the other hand, has a low selection power. The second (bottom) figure displays TPR/FDR values relative to non-zero components of , i.e. the cross-correlated components. We can observe that MT-CoReg performs as well as sCCA, while Lasso is unable to properly select the components of interest. It is encouraging to see that MT-CoReg takes the best of both methods and seems to properly select the components we are interested in. It seems to confirm our hypothesis that using a mix of both terms may lead to an improved feature selection accuracy. In the next section, we apply the same method to a real dataset of fMRI and SNP data.
4.2. Results on real imaging genetics data
4.2.1. Data acquisition
Both SNP and fMRI acquisition were conducted by the Mind Clinical Imaging Consortium (MCIC) for 214 subjects, including 92 schizophrenia patients (age: 34 Â± 11, 22 females) and 116 controls (age 32 Â± 11, 44 females). Schizophreniac were diagnosed based on DSM-IV-TR citeria. Controls were free of any medical, neurological of psychiatric illnesses.
fMRI were acquired during a sensor motor task with auditory simulation. Data were pre-processed with SPM5, spatially normalized and resliced, smoothed, and analyzed by multiple regression considering the stimulus and their temporal derivatives plus an intercept term as regressors. For each patient, a stimulus-on vs. stimulus-off contrast image was extracted. 116 ROIs were extracted based on the aal brain atlas, which resulted in 41236 voxels left for analysis. SNP data were obtained from blood sample using Illumina Infinium HumanOmni1-Quad array covering 1,140,419 SNP loci. After standard quality control procedures using PLINK software packagea, a final dataset spanning 777, 635 SNP loci was available. Each SNP was categorized into three clusters based on their genotype and was represented with discrete numbers: 0 (no minor allele), 1 (one minor allele) and 2 (two minor alleles). SNPs with > 20% missing data were deleted and missing data were further imputed. SNPs with minor allele frequency < 5% were removed. This procedure yielded a final set of 129, 145 SNPs.
4.2.2. Significance analysis
In order to achieve a stable feature selection process, we follow Lin and perform N = 100 random samplings out of the 214 total subjects, where for each time 80% are used for training and parameter selection, while the remaining 20% are used for evaluation. At the k âˆ’ th random sampling, we can calculate a set of solution vectors . It is then possible to Define a measure of relevance  for the i-th feature in the m-th dataset such that:  where i = 1, â€¥, dm is the feature index and I(Â·) is the indicator function. We can then rank each SNP and voxel based on their associated relevance measure and apply a cut-off threshold of 0.3 (c.f. Lin). After applying this Significance test, we were left with a subset of 43 SNP spanning 30 genes and 6 ROI with a number of selected voxels over 5. We display in Table. 1 the list of each of the 43 selected SNP, as well as their associated genes. Some of them have been identified by other similar studies such as CNTNAP2, GLI2, GRIK3, NOTCH4, SUCLG2, GABRG2. Others have been identified from well-known databases such as GRIK4 or HTR4. We display in Table. 2 the list of the selected ROI as well as the corresponding voxel count for each one of them. ROI for which less than 5 voxels were selected where dismissed. Once again, it is encouraging to note that each of the selected ROI (3, 7, 11, 40, 51, 100 from aal.) have been identified in similar studies on the same dataset. Other studies pointed out both functionnal or structural differences in the middle occipital gyrus and the parahippocampal gyrus for schizophrenic patients. Finally, a detailled slice view of the selected voxels can be seen in Fig.(4).
4.2.3. Quantitative analysis
In this section, we try to analyze the results of MT-CoReg using some quantitative metrics. We can first turn our attention to the Sum of Squared Errors (SSE) values obtained on the testing set during our tests. Histograms of SSE distributions for different Î³ values (i.e. Lasso, MT-CoReg and sCCA) can be seen in Fig.(5, left): unsurprisingly, Lasso and MT-CoReg produce the lowest RSS values, while sCCA does not fit the phenotype. If we now look at Fig.(5, middle) where distributions of Pearsonâ€™s correlation on the testing set are displayed for the same 3 strategies, we can see that MT-CoReg produces a better selection than Lasso in terms of cross-correlation. This seems to confirm our intuition that MT-CoReg makes the best of both Lasso and CCA by producing a solution that is good fit to the phenotype while selecting co-expressed features across modalities.
Distribution of Î³ values produced by the consistency selection scheme described in Section 3.3 can be seen in Fig.(5, right). Most of these values fall into the range [0; 0.4], with a peak in [0.2; 0.3]. It does appear, at least in term of feature consistency selection, that a non-zero weight for the CCA term in Eq.(3.3) leads to improved performances.
5. Conclusions
The main contributions of this paper can be summarized as follows. First, we proposed a novel variable selection approach using a CCA-like regularization term in order to enforce co-expression between modalities. Secondly, we present an efficient algorithm to solve this problem, as well as strategies to estimate the tuning parameters. On top of that, a series of experiments on both synthetical and real datasets were conducted, allowing us to evaluate the performances of the proposed method. We identified two sets of SNP and voxels in which a number of them have been previously reported to have potential relationship with the risk of schizophrenia. Further exploration of the optimization scheme (alternate estimations) as well as the selection of regularization parameter Î» (see Section 3.3) will be needed in the future.
           http://pngu.mgh.harvard.edu/purcell/plink         
References































Schematic view of the MT-CoReg pipeline. From two different datasets X1 and X2 from different modalities (here SNP and fMRI respectively), we fit both a regression and CCA terms and couple the resulting components (Î±m, Î¸m) using the â„“1/â„“2 norm denoted â€–Â·â€–1,2 here. The ultimate goal is to find discriminative SNP and brain regions that are also co-expressed across modalities.
Resulting estimates Î²1, Î²2 on the toy dataset. (Truth) blue and red curves are the values taken by the true canonical and regression coefficients respectively. Solutions obtained with sCCA, Lasso, CoReg and proposed method MT-CoReg are displayed. Notice that columns columns p = [10, â€¥ 15] correspond to both non-zeros values in the true regression and canonical coefficients, although their amplitudes are different. By relaxing the assumption that regression and canonical coefficients have identical values, MT-CoReg allows a finer joint estimation of both components types compared to CoReg.
TPR against FDR values averaged over 100 simulations for different Î³ values. Fixing Î³ = 0 amounts to using Lasso regression, while Î³ = 1 is equivalent to classical sparse CCA. Î³(C.S.) is the ROC curve obtained while using consistency selection (C.S.) scheme described in section 3.3 to automatically estimate Î³. (a) values for the selection of first 100 components (i.e. the explanatory components) only (b) values for the selection of the last 100 components (i.e. the cross-correlated components). It can be seen that a non-trivial weight combinaison for Î³ seems to be taking the best of the two methods that are Lasso (Î³ = 0) and CCA (Î³ = 1).
Slice view of the selected voxels (without thresholding using cluster size) and their Significance.
Frequency distribution of RSS values (on the test set) for N = 100 sub-sampling of the original set of observations.
List of selected SNP and their associated genes.
SNP ID	Gene name	SNP ID	Gene name	SNP ID	Gene name	SNP ID	Gene name	 	rs3856465	ATP6V1C2	rs11607732	GRIK4	rs815533	CACNA2D3	rs10748732	HPSE2	 	rs12333931	CNTNAP2	rs12332417	HTR4	rs2373347	CNTNAP2	rs13359903	HTR4	 	rs2407264	CYSLTR2	rs7725785	HTR4	rs9535112	CYSLTR2	rs11875988	LIPG	 	rs6567629	DHRSX	rs12454370	LIPG	rs858341	ENPP1	rs9787820	LRRC4C	 	rs16842460	EPHB1	rs17819648	MAML2	rs11927660	FGF12	rs3134797	NOTCH4	 	rs17599845	FHIT	rs3134799	NOTCH4	rs10926254	FMN2	rs394657	NOTCH4	 	rs4659573	FMN2	rs1009708	PDE2A	rs11060822	FZD10	rs7111188	PDE2A	 	rs12824777	FZD10	rs17016738	RARB	rs2963094	GABRG2	rs12101383	SMAD6	 	rs10831614	GALNTL4	rs7030433	SMARCA2	rs7602673	GLI2	rs573700	SPRY2	 	rs6753202	GPD2	rs9849270	SUCLG2	rs1392744	GRIK3	rs1105880	UGT1A6	 	rs10502240	GRIK4	rs17863787	UGT1A6					 	
List of selected ROI (from aal.) and associated voxel count.
ROI ID (aal.)	ROI name	voxels nb.	 	51	Left middle occipital gyrus	13	 	7	Left middle frontal gyrus	11	 	11	Left middle frontal gyrus, orbital part	9	 	100	Right lobule VI of cerebellar hemisphere	9	 	3	Left superior frontal gyrus	8	 	40	Right parahippocampal gyrus	7	 	
