The Role of Diversity in Data-driven Analyses of Multi-subject fMRI Data
Data-driven methods have been widely used in functional magnetic resonance imaging (fMRI) data analysis. They extract latent factors, generally, through the use of a simple generative model. Independent component analysis (ICA) and dictionary learning (DL) are two popular data-driven methods that are based on two different forms of diversity—statistical properties of the data—statistical independence for ICA and sparsity for DL. Despite their popularity, the comparative advantage of emphasizing one property over another in the decomposition of fMRI data is not well understood. Such a comparison is made harder due to the differences in the modeling assumptions between ICA and DL, as well as within different ICA algorithms where different algorithms exploit different forms of diversity. In this paper, we propose the use of objective global measures, such as time course frequency power ratio, network connection summary and graph-theoretical metrics, to gain insight into the role that different types of diversity have on the analyses of fMRI data. Four ICA algorithms that account for different types of diversity and one DL algorithm are studied. We apply these algorithms to real fMRI data collected from patients with schizophrenia and healthy controls. Our results suggest that no one particular method has the best performance using all metrics, implying that the optimal method will change depending on the goal of the analysis. However, we note that there is no scenario where the popular Infomax has the best performance, demonstrating the cost to of exploiting fewer forms of diversity.
Introduction
In blind source separation problems, such as functional magnetic resonance imaging (fMRI) data analysis, it is beneficial to summarize the observed data through a latent factor model. However, little is known about the processes underlying the generation of the factors. This motivates the development of data-driven methods, which extract latent factors generally through the use of a simple generative model. Data-driven methods have proven useful for the analysis of fMRI data –. Different forms of diversity—types of statistical properties—of the underlying sources, such as independence and sparsity, are adopted to achieve the decomposition, thus resulting in the development of different data-driven techniques.
One such technique that has proven quite popular is independent component analysis (ICA) that takes (statistical) independence among the underlying sources into account,,. This popularity has, in part, led to the development of different ICA algorithms which are derived from the maximum likelihood principle, each designed to achieve the independent component decomposition through exploiting different forms of diversity of the signals, such as higher-order statistics, noncircularity, and sample dependence –. For example, the Infomax algorithm, which was the first algorithm used for fMRI analysis, exploits higher-order statistics through the use of a fixed tangent hyperbolic non-linearity. In contrast to Infomax, entropy bound minimization (EBM) and entropy rate bound minimization (ERBM) are two more recently introduced ICA algorithms and have been shown to provide desirable performance on both simulated and real fMRI data, see e.g.,, –, through the use of a dynamic nonlinearity, and in the case of ERBM, sample dependence as well. The use of a dynamic nonlinearity enables EBM and ERBM to match a wide variety of distributions.
Besides independence, sparsity, another form of diversity that has been exploited in many fields, has proven to be a useful property in the decomposition of fMRI signals –. An increasingly popular sparsity exploiting method that has become popular for the analysis of fMRI data is dictionary learning (DL),,. Its popularity stems, in part, from the fact that it uses only sparsity to determine functional networks of interest, and that it does not require the modeling of a source distribution. Instead, it aims at balancing the decomposition accuracy and source sparsity through a regularization parameter.
Due to the desirable performance of methods that take either sparsity or independence into account, a unified mathematical framework that enables dynamic exploitation of both independence and sparsity has been proposed, and it is referred to as sparse ICA framework. Source sparsity is incorporated through the use of the ICA cost function, penalized by an ℓ1 regularization term.
Although independence and sparsity have demonstrated their utility for the analysis of fMRI data, there is no study that explores the role of each diversity in terms of global metrics for real fMRI datasets. Those few studies that have investigated the performance of Infomax, EBM and ERBM, used a limited number of subjects and based the evaluation on subjective metrics, such as visual inspection of a few well-matched components,. Additionally, there has been no comparison of the performance of these methods with sparse ICA framework and DL. Such an exploration raises the issue of how to determine a metric for comparing the performance of different data-driven algorithms on real fMRI data without a ground truth. Algorithmic comparison is difficult as decompositions can be quite different depending on the modeling assumptions of the particular algorithm, thus matching of all the estimated components is usually not possible. This motivates the use of more objective global metrics for algorithmic performance.
The purpose of this work is to gain insight into the role of different types of diversity on a decomposition. For this, Infomax, EBM, ERBM, DL and sparse ICA framework are studied. Additionally, the relative performance of the different algorithms is assessed on a large real fMRI dataset consisting of 179 subjects. Due to the increasing number of large fMRI datasets that include hundreds or even thousands of subjects, understanding the performance of these techniques in this scenario increases our confidence in the generalizability of the results. In order to be fair to each algorithm, we propose to use objective global measures, e.g., time course frequency power ratio,, network connection summary, and graph-theoretical metrics,. Time course frequency power ratio indicates whether a component is describing the blood-oxygenation-level-dependent (BOLD) response in fMRI data or not. Network connection summary gives a general idea on how well each algorithm can reconstruct the complex connections in brain. Graph-theoretical metrics are an efficient tool for studying the heterogeneity between different groups of subjects, such as patients with schizophrenia (SZs) and healthy controls (HCs) –. These metrics perform a global comparison of the algorithms based on all the components or the whole brain functional connectivity.
Through this comparison, we find that the use of global metrics for a performance comparison can provide a general guide to the practitioners about the selection of the appropriate algorithm for a specific situation. For instance, DL produces components comprising signal that is more likely to be derived from the BOLD response, EBM yields better clustering within functional networks, and ERBM is better in capturing group differences and yields higher variance in SZs than HCs when using graph-theoretical metrics.
The rest of the paper is organized as follows. In Section II, we introduce the data that is used, describe the group ICA framework as well as the four ICA algorithms, DL, and describe the three global measures in detail. In Section III, we present the experimental results. We discuss the results in Section IV and conclude with Section V.
Materials and methods
Data acquisition
The data used in this study is a resting state fMRI data from the Center of Biomedical Research Excellence (COBRE), which is available on the collaborative informatics and neuroimaging suite data exchange repository (http://coins.mrn.org/dx),. The data includes 88 SZs (average age: 37 ± 14) and 91 HCs (average age: 38 ± 12). All images were collected on a single 3-Tesla Siemens Trio scanner with a 12-channel radio frequency coil using the following parameters: TE = 29 ms, TR = 2 s, flip angle = 75°, slice thickness = 3.5 mm, slice gap = 1.05 mm, voxel size 3.75 × 3.75 × 4.55 mm3. Participants were instructed to keep their eyes open during the scan and stare passively at a central fixation cross. Each resting state scan consists of 150 volumes. To eliminate the T1-related signal fluctuations (T1 effect), the first 6 volumes are removed in this study, thus 144 volumes remain for each subject. The fMRI data are realigned with INRIalign algorithm, slice-timing correction is applied using the middle slice as the reference frame in the functional data pipeline and spatially normalized to the standard Montreal Neurological Institute space and resampled to 3 × 3 × 3 mm3, resulting in 53 × 63 × 46 voxels. Afterwards, the fMRI data are smoothed using a Gaussian kernel with a full-width at half-maximum of 10 mm.
Group ICA
The group ICA framework enables analysis of fMRI data from multiple subjects. Let the observed fMRI data from the kth subject be denoted by , 1 ≤ k ≤ K, where T denotes the number of time points and V denotes the number of voxels. To reduce the contamination from noise, principal component analysis (PCA), using an order suggested by the entropy rate based order selection technique described in, is employed to reduce the dimension of the data for each subject. The order estimation method proposed in takes sample dependence into consideration without downsampling, which leads to improved estimation of the signal subspace. For each subject, the dimension of  is reduced from T to T′, computed as , where † represents the pseudoinverse and  is the subject level reduction matrix, whose columns are the eigenvectors of  and the reduced data is . The reduced data matrix, X[k], consists of the first T′ principal components of  that represent the informative signals from the kth subject. It is assumed that the subjects share a common component subspace. In order to estimate the common signals across subjects, the datasets are temporally concatenated to form a single data matrix , which is then reduced to  by a group level PCA, , with  as the group level reduction matrix and N as the order for the common observation subspace. Group components  are then estimated by performing ICA on the common group subspace Y,  where  is the mixing matrix. ICA seeks to find a group demixing matrix, W, such that the estimated sources are obtained as . The use of a single ICA on the common subspace of all datasets helps to preserve the order of the components across subjects. Following the completion of ICA, back-reconstruction is performed on  to generate the corresponding subject-specific source estimates . In order to obtain the back-reconstructed signals, , the group level reduction matrix G† is blocked by columns, G† = [(G[1])†, (G[2])†, ⋯, (G[K])†] with  and,  is reconstructed using , and the corresponding subject-specific time courses are obtained using .
Independence-based ICA: Algorithm choice
The differences in separation performance for separate ICA algorithms, such as Infomax, EBM and ERBM, are related to differences in their assumed latent source models. In order to estimate the demixing matrix W, Infomax and EBM equivalently aim at minimizing the mutual information between the source estimates . The corresponding cost function is  where H(·) = −E[logp(·)] refers to (differential) entropy of a random variable, with p referring to the probability density function (PDF) of corresponding variable. The last term H(Y) is a constant as it is independent of W. Estimation of the PDF of sources is a crucial task. To resolve this task, Infomax takes only the diversity of higher-order statistics into consideration using a distribution model implied by a fixed sigmoidal nonlinearity. This fixed nonlinearity is a good match for very focal regions of activation, however it might significantly bias latent sources relating to broad regions, such as the default mode network (DMN). In contrast to Infomax, EBM does not assume one specific distribution for the latent sources but instead attempts to upper bound their entropy through the use of several measuring functions. Each of these functions provides bounds on the entropy, with the tightest bound being closest to the true entropy. The use of these measuring functions makes it possible to match a wide variety of distributions, including those that are sub-Gaussian, super-Gaussian, unimodal, bimodal, symmetric, as well as skewed, thus potentially leading to more accurate estimation of the latent sources.
Instead of bounding the entropy of latent sources, ERBM attempts to bound their entropy rate using measuring functions. The cost function, thus, is given by  where  refers to the entropy rate of , thus the cost function becomes the mutual information rate between the source estimates. Note that the third term Hr (Y) is a constant and wont be counted in optimization. By calculating the entropy rate of sources, one more type of diversity, namely, sample dependence, is taken into consideration. Since EBM and ERBM relax the assumptions placed on the fMRI sources by assuming flexible source distributions, they are expected to provide improved performance over Infomax. Additionally, ERBM is expected to have superior performance over EBM as well as Infomax, since it takes advantage of the underlying properties of the fMRI components, namely, higher-order statistics and voxel-wise dependence.
Sparsity-based DL
DL is one of the data-driven method that only takes the sparsity of latent sources into consideration. Recently, it has been successfully applied to fMRI data analysis,. DL expresses the observations as sparse combinations of the atoms (columns) in a dictionary , seeking to estimate the latent sources  that is conveyed in the sparse loadings. The cost of DL procedure is given by  where  are the spatial maps, , and λ is the regularization parameter. The DL algorithm used in this work achieves the decomposition using an online optimization algorithm, making it suitable for large datasets with millions of samples.
Balancing independence and sparsity
Due to the desirable performance of methods that take either sparsity or independence into account, recently, a unified mathematical framework is proposed to account for both independence and sparsity of sources by incorporating a sparsity term into the cost function of ICA,. EBM is utilized to demonstrate its application and the corresponding algorithm is referred to as SparseICA-EBM. The cost function of SparseICA-EBM is constructed of two terms, i.e., an independence term and a sparsity term. The independence refers to (2) and the sparsity is written as the summation of the regularization function of all estimates. The final cost function is given by   where  is the regularization term with respect to the nth source estimate , λn is the sparsity parameter and ϵn is the smoothing parameter. The third term in (2) is removed here since it is a constant with respect to W. From the cost function we can see that different sparsity and smoothing parameters enable different estimates.
Parameter selection
One important parameter in the application of an ICA algorithm is the model order, i.e., the number of common signals, N. However, for fMRI data, classical order estimation techniques based on information theoretic criteria may overestimate the order due to the inherent sample dependence of fMRI data,. A common way to overcome this issue is by using downsampling to obtain effectively independent and identically distributed samples,. Unfortunately, methods based on downsampling suffer from a loss of information associated with the downsampling. More recently, two entropy rate (ER)-based order estimation techniques are proposed that account for sample dependence without the use of downsampling: ER using a finite memory length model (ER-FM) and ER using an autoregressive model (ER-AR). Therefore, these two methods are used in this paper to estimate the number of common signals.
As introduced above, SparseICA-EBM seeks to achieve the decomposition by assuming both independence and sparsity using a sparsity parameter λn, which enables the sparse solution for the nth source, . The balance between independence and sparsity can be adjusted by tuning λn. Smaller λn in (5) emphasizes independence more, and larger λn places greater weight on sparsity. Another parameter for SparseICA-EBM is the smoothing parameter ϵn. In (5), the regularization term of sparsity was originally , which is non-differentiable. To resolve this issue, it is replaced by the sum of multi-quadratic functions, as given by (6). Higher smoothing parameter ϵn will help to produce smoother sources. However, the effect of this parameter with application to fMRI data analysis has not been explored. For this reason, in this work, different values of λn and ϵn are considered.
Similarly, an appropriate value should be set for the regularization parameter λ for DL. The analytic link between λ and the corresponding effective sparsity of  is not clearly investigated. The performance of DL is highly dependent on the selection of λ for the tradeoff between accuracy and sparsity. There has been no clear guidance for the choice of λ in real world applications. Hence, in this work, we investigate the influence of different values of λ on the performance for our relatively large real fMRI data. There is another key parameter for DL in practice, the number of atoms in dictionary. As we are seeking to compare the performance of ICA algorithms and DL algorithm, to make it easier for implementation, we set the atom number to be the same with the number of latent sources that is estimated for ICA.
Global metrics for performance evaluation
To compare the performance of these five algorithms, proper measures are needed. Since it can be difficult to exactly match all of the estimated components across different algorithms, it is hard to directly compare algorithmic performance on real fMRI data. In order to resolve this issue, we propose the use of global measures to compare the performances of data-driven algorithms on real data. Following are three global measures that are used.
Frequency analysis:
The first global measure of data-driven algorithmic performance on real fMRI data is the ratio of time course power spectra in low-frequency band (< 0.1Hz) to the high-frequency band (> 0.15 Hz) for each IC. Since the activation in the components is due to the BOLD response, which corresponds to the low frequencies, the higher power ratios implies that the components are more closely associated with true neural function. Conversely, the lower the ratio, the more likely the component is to be describing cardiac or respiratory noise as opposed to true BOLD activation,.
Network connection summary:
Another global measure of the performance is network connection summary. Prior to the construction of network connection summary, for each algorithm, M ICs out of N that refer to brain functional networks are selected based on their time course power ratio and visual inspection. First, all the components are separated into two categories, one containing those with time course power ratio higher than 3, and the other containing the remaining ones. After that, through visual inspection the components with large edge effects and ventricles are removed from the first category, and those with low power ratio but obviously refer to meaningful network are put back to the first category, which is finally used for network connection analysis. Using time course power ratio to separate components into two categories in the first step will significantly reduce the difficulty of visual inspection in the second step.
For ICA algorithms, though they assume that the latent sources are fully independent, following the performance of ICA the extracted ICs generally have some dependence due to their functional relevance. Consequently, after identifying the functional networks that are conveyed in the ICs, we estimate the brain connectivity by calculating the full-order statistical dependence, mutual information (MI), among them. The functional connectivity is constructed using normalized MI, , as the measure. The normalized MI is given by  where  is the MI between two estimated components  and . The performance of three Shannon entropy based MI estimation methods, k-nearest neighborhood, analytical formula corresponding in the chosen exponential family and Parzen window based method is investigated. The former two methods are from information theoretical estimators (ITE) toolbox (https://bitbucket.org/szzoli/ite/). Our exploration shows that for super-Gaussian variable, the method proposed in performs the best, and it is used in our work. After the normalization, the connectivity between two similar sources would be close to 1 and that of two dissimilar sources would be close to 0.
Functional networks identified from these data-driven algorithms are expected to have the ability to reconstruct the brain network connections. A better connection reconstruction shows better decomposition performance and will improve the results of post analyses, such as clustering, on these estimated networks. To measure the modularity of the functional networks, the ratio of the average intra-module connectivity to the average inter-module connectivity is defined as:  where Q is the number of modules, Ni is the number of connections within the ith module, Ni,j is the number of connections between the ith and jth modules, eu and ev refer to the intra- and inter-module connectivity respectively, and Nintra and Ninter refer to the total number of intra- and intermodule connectivity, respectively. The larger the ratio, the more compact the modules are.
Graph-theoretical metrics:
Graph-theoretical metrics are other global measures that are used for performance comparison. For each algorithm and subject, the M selected components are used to construct the graph. With the M ICs of interest as nodes and the normalized MI between them as the edges, e, a fully connected graph, G, is constructed. Beginning with G, a edge threshold, et, is used to retain only the highest P percent of the edges, thus generating a new graph G. We define the percentage of the edges that remain after thresholding as the link density, which increases as the threshold decreases. The weighted graphs are converted to binary ones, with the edges below the threshold et having a value of 0 and those above having a value of 1. In order to avoid very sparse graphs with small link densities and those with too large link densities, we limit the link density to range from 20% to 70%.
Graph-theoretical analysis is done on these graphs by calculating different graph metrics. There have been several reviews, see,,, showing that graph metrics highlight different topological characteristics of graphs. The characteristic path length (PL), global efficiency and centrality are measures that can facilitate functional integration in graphs. They are globally calculated, which means that all the other nodes are taken into consideration in their calculation, and provide measures of how information is transferred in the functional network. Clustering coefficient (CC) is a measure that can capture the segregation of networks by measuring the transfer of information in the immediate neighborhood of each node. The global graph CC that is averaged across all the nodes is explored. Small-worldness (SW) of the network is also calculated to measure the degree of small-world organization in the overall functional network. Table. I presents the metrics that are used in this work. The formulas of these metrics are described in detail in –. All the implementations are performed using Matlab codes in the Brain Connectivity Toolbox (https://sites.google.com/site/bctnet/).
Results
Implementation
Model order:
Estimation of the dimension of source subspace is crucial when using linear mixture model like described in (1) with additive noise. Two order estimation methods, ER-FM and ER-AR, are applied to each subject in the COBRE data separately, we find that the mean and standard deviation of the order across subjects are 72.86 ± 10.40 for ER-FM and 77.33 ± 10.91 for ER-AR. Since the sample correlation structure in ER-FM is a better match to that in fMRI data due to the finite span of correlation in the point spread function, we use an order equal to the mean plus one standard deviation estimated using ER-FM, which is rounded up to 85 to retain a significant level of the signal across multiple subjects while introducing minimal noise. The use of this high model order is also well motivated in the literature for achieving a more useful functional segmentation of the brain, see e.g.,,.
Finally, in group ICA, the order of subject-level PCA is set to 100, a little higher than 85, seeking to remain as much variability as possible. In DL, we first try to determine a decomposition without any dimension reduction but the estimates are very noisy regardless of the parameter values. For consistency with group ICA, we perform subject-level PCA using the same order with group ICA, then perform DL on the concatenated data and estimate 85 components.
Back-reconstruction:
Direct group ICA (GICA) back-reconstruction is done to obtain the subject-specific time courses and functional networks for ICA algorithms. While the second compression in DL is not explicitly known, only the indirect back-reconstruction approach, spatio-temporal regression (STR, or dual regression),, can be adopted. To make the estimates comparable, GICA1, a GICA back-reconstruction method that partitions the group-level PCA reducing matrix G† as described in Section II–B, is used to perform back-reconstruction for ICA, since it has been shown to have similar performance with dual regression.
The most stable run is selected using minimum spanning tree method for EBM from 10 runs and for ERBM and SparseICA-EBM from 25 runs. Multiple runs are not performed for either Infomax or DL due to the consistency of their decompositions. Using back-reconstruction, the 85 ICs are estimated for individual subject associated with their time courses. All the analyses are performed on the 85 mean ICs that are generated by averaging the back-reconstructed ICs across all subjects.
SparseICA-EBM:
As introduced above, there are two key parameters for SparseICA-EBM, the regularization parameter λn and the smoothing parameter ϵn. In this work, the regularization and smoothing parameters are kept the same for all estimates, thus the index n is dropped. We first try to find a suitable value of λ for our dataset by fixing ϵ = 10. As noted in, the performance trend of SparseICA-EBM is very similar among different values of ϵ. This makes our strategy of fixing ϵ when exploring λ reasonable. A suitable value of λ is chosen based on the cost function, spatial maps and time course power ratio. Further, with the selected λ, different values of ϵ are used to find out the one that gives the most reasonable results.
In order to select λ, a wide range of values from 10−8 to 104 are used to study the contribution of independence and sparsity in the sparse ICA cost function. As shown in Fig. 1(a), when λ = 10−5, independence contributes more and when λ = 10−4, sparsity contributes more. For 10−4.5, the contribution of independence and sparsity is balanced as seen in Fig. 1(b). The spatial components are visually inspected for different values of λ. We notice that the decomposition fail to estimate all possible latent sources when λ is too large, such as 102 and 104. The estimated sources are very focal, which means that the activated area of this estimate is compact with little noise and similar components are estimated multiple times. However, this comes at the cost of not estimating sources whose activated area is expected to be larger or dispersive, such as the DMN and fronto-parietal, which are reported to be of interest in resting-state fMRI data,. When λ is too small, as 10−6 and 10−8, the decomposition become very similar to that of EBM, i.e., sparsity scarcely contributes in these cases.
The frequency power ratio of time course for different values of λ is shown in Fig. 1(d). Only those values of λ that showed reasonable spatial maps are investigated. The black curve is the smoothed distribution of ratio within all subjects. The box plot displays the median, the 25th and 75th percentiles of the time course power ratio with whiskers extending to the 99.3% confidence interval and some outliers beyond whisker. The mean and standard deviation are in magenta. Horizontal magenta and blue lines refer to the global average, 4.20, and median power ratio, 3.50, across ICs from all decompositions, respectively. From Fig. 1(d), we can see that when λ = 10−4.5, the independence and sparsity is balanced, the decomposition does not give the best statistics for power ratio summary. Even though that when λ = 10−5 (with mean as 4.55 and median as 3.58), the decomposition gives slightly better power ratio summary than that when λ = 10−4 (with mean as 4.42 and median as 3.57), the components from λ = 10−4 look much better than those from λ = 10−5. Therefore, we decide to use λ = 10−4, in which case that sparsity contributes slightly more than independence.
To explore the influence of ϵ, we fix λ = 10−4 and change the value of ϵ from 10−2 to 102. By visual inspection, we notice that when using small ϵ, some components show up repeatedly in the decomposition. The repeated components are those very focal ones and have good power ratio. Though this leads to slightly better power ratio statistics, they are not reliable. For larger ϵ, both the spatial maps and the power ratio statistics are better when ϵ = 102. Therefore, we decide to use λ = 10−4 and ϵ = 102 for SparseICA-EBM during our next experiments.
DL:
There is only one regularization parameter λ for DL. It controls the balance between decomposition accuracy and sparsity. Different values, from 10−2 to 102, are explored to study its effect. As mentioned before, we use the same number of common signals, 85, used in ICA as the number of atoms in DL. For each λ, we checked both the spatial components and the time course frequency power ratio statistics. Components estimated from smaller λ are much noisier than those from larger λ, which can be seen from the examples in Fig. 2. The activated area of the spatial maps from λ = 102 is a little smaller than that from λ = 101. Additionally, components from λ = 102 are not so well-aligned with those from the other values of A, and some of them seem to split, such as the fronto-parietal. Power ratio statistic obtained for each λ is shown in Fig. 3(a). Notice that when λ = 101, both the mean, 4.83, and median, 4.12, are much higher than the global mean, 3.76, and median, 3.33. It comes out that when λ = 101, the decomposition performs the best, and hence we use this value for DL in our further experiments.
Power ratio comparison
Spatial maps of some mean ICs from each algorithm are shown in Fig. 4. They are just a few of the estimated components that can be easily matched. For those functional networks that only contain one single relevant area, all five algorithms produce a good decomposition. However, for the DMN component, EBM and ERBM estimate more relevant activated areas since it uses a more flexible density estimation model. Infomax only uses a fixed unimodal super-Gaussian density model which can hurt the quality of its decomposition. Consequently, the components estimated by Infomax are all with very focal activated areas and have very similar PDFs. SparseICA-EBM and DL emphasize the sparsity of the sources, which makes the estimated components tend to have fewer activated voxels outside the regions of interest. However, components with large functional regions, such as DMN and fronto-parietal, cannot be approximated properly by them. The time course power ratio comparison of the 85 mean ICs among the five algorithms is shown in a violin plot in Fig. 3(b). Horizontal magenta and blue lines refer to the global average, 4.52, and median power ratio, 3.53, across ICs from all five algorithms, respectively. We can see that DL yields both the highest mean, 4.83, and median, 4.12. The power ratio statistics shows a small trend of increasing from Infomax, EBM, ERBM, SparseICA-EBM to DL. This implies that emphasizing sparsity results in components that are more likely to be related to BOLD signal.
Network connection summary
In this experiment, M = 50 comparable components that refer to brain functional networks are selected for each algorithm. Network connection summary of the 50 mean ICs is created for each algorithm using the normalized MI as a measure. We group these components into six basic clusters, motor, cognitive control (COG), default mode (DM), auditory (AUD), visual (VIS) and cerebellum (CB), according to their anatomical and presumed functional properties. Fig. 5 shows the composited spatial maps for each cluster, the functional connectivity matrix and the network connection summary for EBM. The functional connectivity matrix exhibits some patterns of the brain network connection, for example, the modular organization within motor, DM and so on. These patterns are consistent with the observations in prior literature,. The connection summary visually illustrates these patterns. The intra- and inter-cluster connectivity ratio R is 2.4, 3.2, 1.9, 3.1 and 1.7 for Infomax, EBM, ERBM, SparseICA-EBM and DL, respectively. This reveals that EBM and SparseICA-EBM yield better clustering within each functional network cluster rather than across.
Graph-theoretical analysis
Graph-theoretical analysis is done on the selected 50 ICs for each subject. The graph, G, is formed, where the retained components are nodes and pairwise normalized MI between the spatial components forms the edges. For each binarized graph, G′, both the nodal and global metrics are calculated. Permutation testing, is performed on nodal metrics to obtain the corrected p-value to detect the significance level (p < 0.05) of difference between SZ and HC group. For each nodal graph metric, 10,000 random permutations are generated independently. We first perform two-sample t-test on the graph metric based on the SZ and HC group, and the test statistic, t0, is recorded. To implement the permutation test, all the subjects are randomly divided into two groups. Two-sample t-test is performed on each division and all the test statistics are stored. Finally, the p-value is calculated by counting the number of permutations for which the test statistic is greater than t0 and normalizing by 10,000. In order to prevent nodes from being declared significant by chance, only those nodes that show significant differences in at least three successive graphs are declared to be truly significant.
Our results reveal that for the globally calculated nodal graph metrics, i.e., degree, characteristic PL, global efficiency and three versions of centrality, more components from ERBM show consistent significance in graphs for the majority of link density values, which gives us a greater ability to explore the differences between the two groups. Fig. 6 shows the plot for the number of significant components at each link density for PL and betweenness centrality, and the plots for the other nodal graph metrics are similar with these two (not shown here due to the space limitaion).
Fig. 7 shows the comparison of global graph metrics. It compares the standard deviation (STD) of CC and SW between SZs and HCs. Usually, higher variability is expected for SZs than for HCs in fMRI analyses. The normalized STD difference is calculated by first subtracting the STD of HCs from that of SZs then normalizing it by the STD of HCs. The result reveals that ERBM yields the highest contrast between SZs and HCs using both CC and SW.
Discussion
In this work, ICA model order is estimated using ER-FM, which uses a finite memory length to model the sample dependence and is a good match for fMRI data. In DL, it turns out that the estimates are very noisy without dimension reduction regardless of the parameter values. This illustrates that DL is not able to separate signal components from noise components effectively. Thus, it is vital to perform dimension reduction, such as PCA, to get rid of certain level of noise. Hence subject-level PCA with the same order in ICA is performed before doing DL decomposition.
The influence of parameters in SparseICA-EBM and DL is investigated. The work in shows that in fMRI-like data, SparseICA-EBM with large λ and ϵ yields the best performance, which is evaluated by calculating the correlation between the estimate and ground truth. However, our experimental results from real fMRI data show that when λ is too large, components are estimated repeatedly. Some important components, such as DMN and fronto-parietal, are missed. Large e appears to produce good performance, which is consistent with the results in. For DL, our results suggest that λ = 10 yields the best estimation.
From the comparison results using time course power ratio, network connection summary and graph-theoretical analysis, we can see that different algorithms stand out in different cases. This can be explained by the intrinsic variance of these decompositions which are depending on the dissimilar modeling assumptions. Our results provide the practitioners a general guidance that which algorithm to choose for different purpose.
DL provides better identification of the components that describe the BOLD response. In Fig. 3(b), the power ratio statistics of the components from DL are higher than those from the other algorithms, which illustrates that DL has the capability to estimate better components that are of high possibility to be describing the BOLD response. However, Fig. 4 shows that the activated area in components that are estimated from Infomax, SparseICA-EBM and DL are very focal and localized. Those components, such as DMN and fronto-parietal, that have multiple relevant regions are not completely estimated by these three algorithms. It is known that DMN contains multiple areas, the ventral anterior cingulate cortex (vACC), posterior cingulate cortex (PCC), left and right inferior parietal cortex (IPC). Usually, PCC is estimated together with the left and right IPC, and vACC is estimated in another individual component called anterior DMN. The DMN components from Infomax, SparseICA-EBM and DL are only activated in PCC. Similar observations are noted for fronto-parietal network where ERBM yields a fronto-parietal component with both the frontal and parietal region activated. Consequently, EBM and ERBM provide better estimation of functionally relevant components.
Fig. 5 shows the network connection summary of EBM and it illustrates that components from EBM can help to reconstruct a good functional connectivity of brain. The reconstructed connection from EBM shows dense connectivity within functional networks and also demonstrates reasonable inter-connectivity among different functional networks. The close-packed connectivity within functional networks might provide valuable clues for the development of those automatic clustering algorithms for estimated components.
In the event that you are analyzing the data from two groups and seeking to find out some group differences, ERBM is more preferable. This might because ERBM can produce components with more relevant regions activated thus leading to better graph-theoretical analysis. From the graph-theoretical analysis results shown in Fig. 6, ERBM yields superior capability to estimate more components that show significant group difference when using globally calculated nodal graph metrics, such as degree, PL, global efficiency and centrality. Usually, disconnectivity patterns is common for the people who have a mental disorder. However, healthy people share the same or similar connection pattern in brain. Therefore, high variance is expected in connectivity analysis for patients than that for HCs. In Fig. 7, ERBM demonstrates better ability in capturing the contrast of the variability of SZs and HCs using two global graph metrics, CC and SW.
From our observation, we note that there is no scenario where the popular Infomax has the best performance, demonstrating the cost of exploiting fewer forms of diversity. Nevertheless, there are some limitations in our work. (1) We do not provide a complete parameter selection for all algorithms. E.g., the filter length used in ERBM is a parameter that deserves deeper investigation, while we select a value for it empirically. The actual filter length that can make the samples of fMRI data as independent as possible is hard to determine. It depends on a lot of physical principles and complex brain connection. If more values can be utilized for a further performance investigation, it will be more objective. (2) The best run selection strategy that is used for EBM, ERBM and SparseICA-EBM has not been clearly investigated. There are a lot of crucial issues in best run selection, such as whether to select the most accurate or the most consistent run, which metrics to use for evaluation and if they are reliable. Furthermore, our work is based on the selected run, but the average performance across multiple runs is also of high interest. (3) Reliability test of these metrics should be done for further justification. Even though these metrics have shown their capability to evaluate the algorithmic performance, there is no evidence to show that they are stable for different datasets come from the same group of subjects. As test-retest strategy is now very popular in reliability testing,,, in the future, we can do similar experiments inquiring into the faithfulness of these global metrics in fMRI data analysis.
Conclusion
Data-driven methods that make use of independence and sparsity have proven useful in many applications such as the analysis of fMRI data. This motivates a performance comparison of those algorithms that incorporating independence and/or sparsity on real fMRI data. Consequently, appropriate metrics for performance evaluation are required since the decompositions can be very different and it is usually impossible to match all the estimated components. In this work, we propose to use three global metrics to assess the performance, namely, time course spectra power ratio, network connection summary and graph-theoretical metrics. With the use of time course spectra power ratio, DL yields components that are most likely to describe the BOLD response. By using network connection summary, EBM illustrates better ability to reconstruct the brain connectivity with normalized mutual information as the measure. When graph-theoretical analysis is applied, ERBM demonstrates better capability to capture the group differences between SZs and HCs, especially the higher variance in SZs as expected. This successful application motivates the use of additional global metrics to assess the performance for unbiased algorithmic comparison on real fMRI data analysis, or even on other real data analysis such as electroencephalograph data. Additionally, we should note that there is no scenario where the widely used Infomax has the best performance, which demonstrates that incorporating multiple types of diversity is more desirable.
References
Review of methods for functional brain connectivity detection using fMRI
Exploring the brain network: A review on resting-state fMRI functional connectivity
Joint blind source separation by multiset canonical correlation analysis
A review of group ICA for fMRI data and ICA for joint inference of imaging, genetic, and ERP data
A method for making group inferences from functional MRI data using independent component analysis

Diversity in independent component and vector analyses: Identifiability, algorithms, and applications in medical imaging
The role of diversity in complex ICA algorithms for fMRI analysis
An information maximization approach to blind separation and blind deconvolution
Analysis of fMRI data by blind separation into independent spatial components
Independent component analysis by entropy bound minimization
Blind spatiotemporal separation of second and/or higher-order correlated sources by entropy rate minimization
ICA of fMRI data: Performance of three ICA algorithms and the importance of taking correlation information into account
A graph theoretical approach for performance comparison of ICA for fMRI analysis
Independent component analysis for brain fMRI does not select for independence
A data-driven sparse GLM for fMRI analysis using sparse dictionary learning with MDL criterion
Fast and incoherent dictionary learning algorithms with application to fMRI
Multi-subject dictionary learning to segment an atlas of brain spontaneous activity
Extracting brain regions from rest fMRI with total-variation constrained dictionary learning
Enhancing ica performance by exploiting sparsity: Application to fmri analysis
Sparsity and independence: Balancing two objectives in optimization for source separation with application to fMRI analysis
Performance of blind source separation algorithms for fMRI analysis using a group ICA method
Online learning for matrix factorization and sparse coding
A baseline for the multivariate comparison of resting-state networks
A resting state network in the motor control circuit of the basal ganglia
Modulations of functional connectivity in the healthy and schizophrenia groups during task and rest
Comparison of IVA and GIG-ICA in brain functional network estimation using fMRI data
Complex brain networks: graph theoretical analysis of structural and functional systems
An exploration of graph metric reproducibility in complex brain networks
Functional connectivity and brain networks in schizophrenia
Disrupted modularity and local connectivity of brain functional networks in childhood-onset schizophrenia
Altered resting state complexity in schizophrenia
Simple models of human brain functional networks
COINS: an innovative informatics and neuroimaging tool suite built for large heterogeneous datasets
Thalamus and posterior temporal lobe show greater inter-network connectivity at rest and across sensory paradigms in schizophrenia
Correction for the T1 effect incorporating flip angle estimated by Kalman filter in cardiac-gated functional MRI
What is the best similarity measure for motion correction in fMRI time series?
Statistical parametric maps in functional imaging: A general linear approach
Likelihood estimators for dependent samples and their application to order detection
Multisubject independent component analysis of fMRI: A decade of intrinsic networks, default mode, and neurodiagnostic discovery
Estimating the number of independent components for functional magnetic resonance imaging data
Order detection for fMRI analysis: Joint estimation of downsampling depth and order by information theoretic criteria
Frequencies contributing to functional connectivity in the cerebral cortex in resting-state data
A new class of random vector entropy estimators and its applications in testing statistical hypotheses
A closed-form expression for the Sharma- Mittal entropy of exponential families
Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy
Complex networks: Structure and dynamics
Brain graphs: graphical models of the human brain connectome
Complex network measures of brain connectivity: Uses and interpretations
Power and centrality: A family of measures
Some unique properties of eigenvector centrality
Functional segmentation of the brain cortex using high model order group PICA
Group comparison of resting-state fMRI data using multi-subject ICA and dual regression
Alcohol intoxication effects on simulated driving: exploring alcohol-dose effects on brain activation using functional MRI
Comparison of multi-subject ICA methods for analysis of fMRI data
Functional connectivity in the resting brain: A network analysis of the default mode hypothesis
Assessing dynamic brain graphs of time-varying connectivity in fMRI data: Application to healthy controls and patients with schizophrenia
Tracking whole-brain connectivity dynamics in the resting state
Network-based statistic: Identifying differences in brain networks
Global, voxel, and cluster tests, by theory and permutation, for a difference between two groups of structural MR images of the brain
Investigation of relationships between fMRI brain networks in the spectral domain using ICA and Granger causality reveals distinct differences between schizophrenia patients and healthy controls
Optimization of rs-fMRI pre-processing for enhanced signal-noise separation, test-retest reliability, and group discrimination
Reliability of graph analysis of resting state fMRI using test-retest dataset from the Human Connectome Project
The relative contribution of independence and sparsity (a)–(c) in SparseICA-EBM and the time course power ratio statistics (d).
Comparison of several components from DL using different values of λ. Z-maps corresponding to the mean components averaged across all subjects for DMN, auditory (AUD), motor, sensorimotor (SM) and fronto-parietal (F-P) component are shown. The maps are threshoulded using a threshold Zt = 2. The peak coordinates in mm are shown to the right. Note that the last two fronto-parietal components are decomposed by DL with λ = 102.
Time course power ratio statistics for all 85 mean components of (a) DL using different λ and (b) five algorithms. Note that s-EBM refers to SparseICA-EBM.
Comparison of several components from five algorithms. Z-maps corresponding to the mean components averaged across all subjects for the anterior DMN, DMN and fronto-parietal (F-P) component are shown. The maps are thresholded using a threshold Zt = 2. The peak coordinates in mm are shown to the right. Note that s-EBM refers to SparseICA-EBM.
Network connection summary of components from EBM. (a) Spatial maps of 50 functional networks. Functional networks are divided into groups based on their anatomical and functional properties. COG: Cognitive control; DM: default mode; AUD: auditory; VIS: visual; CB: cerebellum. (b) The functional connectivity matrix. Connectivity is measured using normalized MI. (c) Network connectivity visualization. The outermost circle demonstrates one slice of the spatial map of individual mean component. The colored circle indicates the index corresponding to each component and different colors refer to different functional network clusters. Curves in the center carry the exact connectivity among these components.
Number of nodes showing significant group difference in each graph as a function of link density for (a) characteristic path length (PL) and (b) betweenness centrality. Note that s-EBM refers to SparseICA-EBM.
Comparison of the normalized STD of (a) clustering coefficient (CC) and (b) small-worldness (SW) in SZs and HCs. Note that s-EBM refers to SparseICA-EBM.
Description of graph-theoretical metrics in both node and graph level.
Metric	Abbreviation	Description	 	Nodal metrics (globally calculated)	 	Degree	-	Number of links directly connected to node i	 	Characteristic path length	PL	Average distance of node i to all the others	 	Global efficiency	-	Communication efficiency of node i with all the others	 	Centrality	 	 Betweenness centrality	-	 Prominence of node i in information transfer in network	 	 Closeness centrality	-	 	 Eigenvector centrality	-	 	Global metrics	 	Clustering coefficient	CC	Measure of functional segregation of the network	 	Small-worldness	SW	Quatifying the ability of combining functional integration and segregation	 	
